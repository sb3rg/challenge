{
    "collab_server" : "",
    "contents" : "# ##################\n# ### QUESTION 3 ###\n# ##################\n\n### RESPONSE NOTES ############################\n### The data being generated by 'rnorm(100)' first follows the behavior of a hundred values sampled from a standard normal distribution,\n### where by default rnorm sets parameters mean to zero and the standard deviation to 1.  These hundred samples are returned in a numeric \n### vector in R.  After, the cumulative sum indicated by the 'cumsum()' function creates a running total of \n### the hundred samples.  Graphically the function generates the following:\n###############################################\n\n## BEGIN ANALYSIS AND GRAPHS\nlibrary(ggplot2)\nlibrary(data.table)\nlibrary(magrittr)\n\nx <- rnorm(100)\ny <- x %>% cumsum()\nDT <- data.table(x=x, y=y)\n\nDT %>%\n  ggplot(aes(x=x, y=y))+\n  geom_line()+\n  theme_classic()\n\nggsave(path=\"./\", filename=\"q3_cumsum_random_walk.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n# ggsave(path=\"./\", filename=\"q3_cumsum_random_walk1.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n# ggsave(path=\"./\", filename=\"q3_cumsum_random_walk2.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n# ggsave(path=\"./\", filename=\"q3_cumsum_random_walk3.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n## END ANALYSIS AND GRAPHS\n\n\n### RESPONSE NOTES (continued) ################\n### upon investigating a line graph chart (one example being 'q3_cumsum_random_walk') of the cumulative distribution, the generated data \n### seems to follow a random walk or brownian motion.  Running multiple iterations of 100 samples reveals that the random walk is sensistive\n### to initial conditions. Because the distribution looks at first blush multi-modal, we should investigate the long-run statistics of the \n### generated data at higher values of N.  This effort reveals graphs 'q3_cumsum_dist_100' to 'q3_cumsum_dist_100000'.\n###############################################\n\n\n## BEGIN ANALYSIS AND GRAPHS\nDT %>%\n  ggplot(aes(x=y, y=..count../sum(..count..)))+\n  geom_histogram(binwidth=1)+\n  theme_classic()\n\nggsave(path=\"./\", filename=\"q3_cumsum_dist.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n# ggsave(path=\"./\", filename=\"q3_cumsum_dist_100.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n# ggsave(path=\"./\", filename=\"q3_cumsum_dist_1000.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n# ggsave(path=\"./\", filename=\"q3_cumsum_dist_10000.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n# ggsave(path=\"./\", filename=\"q3_cumsum_dist_100000.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n## END ANALYSIS AND GRAPHS\n\n\n### RESPONSE NOTES (continued) ################\n### More generally, we develop a general sense of statistical parameters for the long-run behavior of the generated data as shown in the\n### following graphs.\n###############################################\n\n\n## BEGIN ANALYSIS AND GRAPHS\n# look at stat params for increasing values of N\nN <- 2:1000\nlist_of_cumsums <- N %>% lapply(rnorm) %>% lapply(cumsum)\nmean <- list_of_cumsums %>% lapply(mean) %>% unlist()\nstd_dev <- list_of_cumsums %>% lapply(sd) %>% unlist()\n\nDT2 <- data.table(N=N, mean=mean, std_dev=std_dev)\n\nDT2 %>%\n  ggplot(aes(x=N, y=std_dev))+\n  geom_point()+\n  geom_smooth()+\n  theme_classic()\n\nggsave(path=\"./\", filename=\"q3_cumsum_std_dev.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n\nDT2 %>%\n  ggplot(aes(x=N, y=mean))+\n  geom_point()+\n  geom_smooth()+\n  theme_classic()\n\nggsave(path=\"./\", filename=\"q3_cumsum_mean.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n\n## END ANALYSIS AND GRAPHS\n\n\n### RESPONSE NOTES (continued) ################\n### In viewing the means and standard deviations with respect to increasing N according to graphs 'q3_cumsum_mean' and 'q3_cumsum_std_dev',\n### respectively, it looks like the long run statistics of the generated data yields a mean that stabilizes around zero, but has INCREASING \n### variance.\n###############################################\n\n\n\n\n\n\n\n# ##################\n# ### QUESTION 4 ###\n# ##################\n\n# set size of samples\nN <- 512  \n\n# create two indices 8 and 16\ninds <- c(8, 16)\n\n# scaling factor for signal amplitude in frequency domain\nb <- 30\n\n# set the number of samples per unit time--sampling frequency\na <- 128\n\n# sample 512 values from the normal distribution\ns <- rnorm(N)\n\n# perform the fast discrete fourier transform of the sample data\n# return a vector of complex numbers representing Real and\n# Imaginary parts.  This converts these values into the frequency\n# domain representation of a theoretical sampling interval\nx <- fft(s)\n\n# repeat false (N/2-1) times.\nlogind <- rep(F, N/2-1)\n\n# replace two falses at index 8 and 16 in the vector\nlogind[inds] <- T\n\n# concatenate logind vector with leading and trailing FALSe values with\n# the added concatenation of the reverse of the original vector\nlogind <- c(F, logind, F, rev(logind))\n\n# replace four complex numbers after scaling by 'b'\n# the indices are now c(9,17,497, 505)\nx[logind] <- b*x[logind]\n\n# take the inverse discrete fourier transform of the scaled response\n# divide the values by half and return only the real component of\n# complex number.\ny <- Re(fft(x, inverse=T)/length(x))\n\n# generate a time series object scaled to a range that's in the\n# standard normal distribution\nresult <- ts(y/sd(y), frequency=a)\n\n## BEGIN ANALYSIS AND GRAPHS\n# install.packages(\"ggfortify\")\nlibrary(ggfortify)\n\nresult %>% autoplot()+\n  ## TITLES & AXES\n  labs( # draw labels\n    y = \"Result\",\n    x = \"Time\"\n  )+\n  theme_classic()\n\nggsave(path=\"./\", filename=\"q4_result_ts.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n  \n## END ANALYSIS AND GRAPHS\n\n### RESPONSE NOTES ############################\n### This is a digital signal processing method.\n### \n### The high-level overview of what's happening with the code chunk is\n### that we're simulating time series data by amplifying certain frequencies\n### in the frequency domain so that when we transform the signal back to the\n### time domain (inverse DFT), the signal has the desired periodicities.\n### The code does this while preserving the original statistical parameters \n### of the sampling distribution.\n### \n### The reason why we have to mirror the signal is to account for aliasing about the\n### nyquist frequency and preserving information about the original signal when \n### performing the inverse transform.\n###############################################\n\n\n\n\n\n\n\n# ##################\n# ### QUESTION 5 ###\n# ##################\nlibrary(caret)\nlibrary(dplyr)\n\ndf <- data.frame(\n  a=c(rnorm(80), rnorm(20,1,2)),\n  b=c(rnorm(80,0,3), rnorm(20,2,2)),\n  c=c(rep(T,80), rep(F,20))\n)\n\n### BEGIN ANALYSIS AND GRAPHS\n\n### STEP 0: assess the types of variables involved in\n### the model.  Here we see that variables 'a' and 'b'\n### are both continuous and variable 'c' is discrete\n### boolean/dichotmous. Given these facts, We will look \n### at logistic regression models for prediction of 'c'.\n### we use the binomial distribution as the link function\n### for the generalized linear model\n\n\n### STEP 1:  partition the data into a training set and \n### a testing set.\nset.seed(12345)\ntrain_idx <- createDataPartition(\n  df$c,\n  p=0.68,\n  list=F,\n  times=1\n)\n\ntrain <- df[train_idx,]\ntest <- df[-train_idx,]\n\n\n\n### STEP 2: create the glm model\nc.glm <- train %>% \n  glm(\n    formula=c ~ a+b, \n    data=.,\n    family=binomial\n  )\n\n\n# STEP 3:  predict how well the training model performs on the testing\n# data.  Choose a threshold for the decision and apply boolean--here\n# we chose at threshold of 51%\ntest$prob <- predict(c.glm, test, type='response')\n\ntest$pred <- test %>% mutate(\n  model\n)\n\n\n### STEP 1: determine if there are any significant relationships\n### between predicted variable 'c' and predictor variables\n### 'a' and 'b'. (aside: we know there shouldn't be as these values\n### were generated psuedo-randomly from independent 'experiments', \n### but we will pretend we don't know this)\nsummary(c.glm)\n### seeing that the p-values are both greater than 0.05, so neither\n### 'a' or 'b' shares a significant relationship to 'c' in the \n### logistic regression model.  With this in mind, we press forward \n### to attempt prediction knowing its accuracy will not be significant.\n\n\n### STEP 2: create new test data and run the model.  Test data shares\n### the same parameters as the starting set and attempt prediction.\nrun_trial <- function(trial_num){\n  # test data\n  new <- data.frame(\n      a = rnorm(1),\n      b = rnorm(1, 0, 3)\n    )\n\n  # predict based on model\n  predict(c.glm, new, type='response')[[1]]\n}\n\n# run 100 independent trials and visualize distribution\ntrials <- 1:100 %>% lapply(run_trial) %>% unlist() %>% data.table(likely_true=.)\n\n# visualize distribution\ntrials %>%\n  ggplot(aes(x=likely_true, y=..count../sum(..count..)))+\n  geom_histogram(binwidth=0.03)+\n  scale_x_continuous(breaks=seq(0,1,0.03))+\n  theme_classic()\n\nggsave(path=\"./\", filename=\"q5_result_glm_trials_dist.png\", height = 4.78, width = 8.75, units = \"in\", device = \"png\" )\n\n\n# summarize descriptive statistics\nsummary(trials)\n\n\n### STEP 3: validate the model and summarize results.\n# because predictor variables 'a' and 'b' do not suggest\n# a significant relationship to 'c,' the model performs\n# poorly in predicting c.\n\n\n\n\n### END ANALYSIS AND GRAPHS\n\n",
    "created" : 1582237491897.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2082490613",
    "id" : "42881126",
    "lastKnownWriteTime" : 1582321510,
    "last_content_update" : 1582321509646,
    "path" : "D://2020-02 SQL Challenge/Questions3-5.R",
    "project_path" : "Questions3-5.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}